## Task 1. Hello world

---

In Cloud Shell enter the following command to run a hello world container to get started:
docker run hello-world

Run the following command to take a look at the container image it pulled from Docker Hub:
docker images

Run the container again:
docker run hello-world

Finally, look at the running containers by running the following command:
docker ps

In order to see all containers, including ones that have finished executing, run docker ps -a:
docker ps -a

This shows you the Container ID, a UUID generated by Docker to identify the container, and more metadata about the run. The container Names are also randomly generated but can be specified with docker run --name [container-name] hello-world.

## Task 2. Build

In this section, you will build a Docker image that's based on a simple node application.

Execute the following command to create and switch into a folder named test.
mkdir test && cd test
Copied!
Create a Dockerfile:
cat > Dockerfile <<EOF

# Use an official Node runtime as the parent image

FROM node:lts

# Set the working directory in the container to /app

WORKDIR /app

# Copy the current directory contents into the container at /app

ADD . /app

# Make the container's port 80 available to the outside world

EXPOSE 80

# Run app.js using node when the container launches

CMD ["node", "app.js"]
EOF
Copied!
This file instructs the Docker daemon on how to build your image.

The initial line specifies the base parent image, which in this case is the official Docker image for node version long term support (lts).
In the second, you set the working (current) directory of the container.
In the third, you add the current directory's contents (indicated by the "." ) into the container.
Then expose the container's port so it can accept connections on that port and finally run the node command to start the application.
Note: Spend some time reviewing the Dockerfile command references to understand each line of the Dockerfile.
Now you'll write the node application, and after that you'll build the image.

Run the following to create the node application:
cat > app.js << EOF;
const http = require("http");

const hostname = "0.0.0.0";
const port = 80;

const server = http.createServer((req, res) => {
res.statusCode = 200;
res.setHeader("Content-Type", "text/plain");
res.end("Hello World\n");
});

server.listen(port, hostname, () => {
console.log("Server running at http://%s:%s/", hostname, port);
});

process.on("SIGINT", function () {
console.log("Caught interrupt signal and will exit");
process.exit();
});
EOF
Copied!
This is a simple HTTP server that listens on port 80 and returns "Hello World".

Now build the image.

Note again the ".", which means current directory so you need to run this command from within the directory that has the Dockerfile:
docker build -t node-app:0.1 .

Now, run the following command to look at the images you built:
docker images

Task 3. Run
Use this code to run containers based on the image you built:
docker run -p 4000:80 --name my-app node-app:0.1

Open another terminal (in Cloud Shell, click the + icon), and test the server:
curl http://localhost:4000

Close the initial terminal and then run the following command to stop and remove the container:
docker stop my-app && docker rm my-app

Now run the following command to start the container in the background:
docker run -p 4000:80 --name my-app -d node-app:0.1

docker ps

Notice the container is running in the output of docker ps. You can look at the logs by executing docker logs [container_id].
Note: You don't have to write the entire container ID, as long as the initial characters uniquely identify the container. For example, you can execute docker logs 17b if the container ID is 17bcaca6f....
docker logs [container_id]

In your Cloud Shell, open the test directory you created earlier in the lab:
cd test
Copied!
Edit app.js with a text editor of your choice (for example nano or vim) and replace "Hello World" with another string:
....
const server = http.createServer((req, res) => {
res.statusCode = 200;
res.setHeader('Content-Type', 'text/plain');
res.end('Welcome to Cloud\n');
});
....
Copied!
Build this new image and tag it with 0.2:
docker build -t node-app:0.2 .

Run another container with the new image version. Notice how we map the host's port 8080 instead of 80. You can't use host port 4000 because it's already in use.
docker run -p 8080:80 --name my-app-2 -d node-app:0.2
docker ps

Test the containers:
curl http://localhost:8080

And now test the first container you made:
curl http://localhost:4000

## Task 4. Debug

Now that you're familiar with building and running containers, go over some debugging practices.

You can look at the logs of a container using docker logs [container_id]. If you want to follow the log's output as the container is running, use the -f option.
docker logs -f [container_id]

You can use docker exec to do this. Open another terminal (in Cloud Shell, click the + icon) and enter the following command:
docker exec -it [container_id] bash
Copied!
The -it flags let you interact with a container by allocating a pseudo-tty and keeping stdin open. Notice bash ran in the WORKDIR directory (/app) specified in the Dockerfile. From here, you have an interactive shell session inside the container to debug.

ou can examine a container's metadata in Docker by using Docker inspect:
docker inspect [container_id]

Use --format to inspect specific fields from the returned JSON. For example:
docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [container_id]
Copied!

## Task 5. Publish

Now you're going to push your image to the Google Artifact Registry. After that you'll remove all containers and images to simulate a fresh environment, and then pull and run your containers. This will demonstrate the portability of Docker containers.

To push images to your private registry hosted by Artifact Registry, you need to tag the images with a registry name. The format is <regional-repository>-docker.pkg.dev/my-project/my-repo/my-image.

## Create the target Docker repository (Using Cloud Console)

You must create a repository before you can push any images to it. Pushing an image can't trigger creation of a repository and the Cloud Build service account does not have permissions to create repositories.

From the Navigation Menu, under CI/CD navigate to Artifact Registry > Repositories.

Click the +CREATE REPOSITORY icon next to repositories.

Specify my-repository as the repository name.

Choose Docker as the format.

Under Location Type, select Region and then choose the location : us-west1.

Click Create.

Configure authentication
Before you can push or pull images, configure Docker to use the Google Cloud CLI to authenticate requests to Artifact Registry.

To set up authentication to Docker repositories in the region us-west1, run the following command in Cloud Shell:
gcloud auth configure-docker us-west1-docker.pkg.dev
Copied!
Enter Y when prompted.
The command updates your Docker configuration. You can now connect with Artifact Registry in your Google Cloud project to push and pull images.

Note: Alternately you can use gcloud CLI for a streamlined command-line approach.

Create an Artifact Registry repository (Using CLI)
Run the following commands to create an Artifact Repository.
gcloud artifacts repositories create my-repository --repository-format=docker --location=us-west1 --description="Docker repository"

Push the container to Artifact Registry
Change into the directory with your Dockerfile.
cd ~/test
Copied!
Run the command to tag node-app:0.2.
docker build -t us-west1-docker.pkg.dev/qwiklabs-gcp-01-f14b2aeb85a7/my-repository/node-app:0.2 .
Copied!
Run the following command to check your built Docker images.
docker images

Push this image to Artifact Registry.
docker push us-west1-docker.pkg.dev/qwiklabs-gcp-01-f14b2aeb85a7/my-repository/node-app:0.2

After the push finishes, from the Navigation Menu, under CI/CD navigate to Artifact Registry > Repositories.

Click on my-repository. You should see your node-app Docker container created:

Test the image
You could start a new VM, ssh into that VM, and install gcloud. For simplicity, just remove all containers and images to simulate a fresh environment.

Stop and remove all containers:
docker stop $(docker ps -q)
docker rm $(docker ps -aq)

You have to remove the child images (of node:lts) before you remove the node image.

Run the following command to remove all of the Docker images.
docker rmi us-west1-docker.pkg.dev/qwiklabs-gcp-01-f14b2aeb85a7/my-repository/node-app:0.2
docker rmi node:lts
docker rmi -f $(docker images -aq) # remove remaining images
docker images

Pull the image and run it.
docker run -p 4000:80 -d us-west1-docker.pkg.dev/qwiklabs-gcp-01-f14b2aeb85a7/my-repository/node-app:0.2

Run a curl against the running container.
curl http://localhost:4000

////////////////////////////

Orchestrating the Cloud with Kubernetes

Google Kubernetes Engine
In the cloud shell environment type the following command to set the zone:
gcloud config set compute/zone us-west1-a

Start up a cluster for use in this lab:
gcloud container clusters create io --zone us-west1-a

You are automatically authenticated to your cluster upon creation. If you lose connection to your Cloud Shell for any reason, run the gcloud container clusters get-credentials io command to re-authenticate.
Note: It will take a while to create a cluster - Kubernetes Engine is provisioning a few Virtual Machines behind the scenes for you to play with!

## Task 1. Get the sample code

To get the code, copy the sample code from a Google Cloud Storage bucket.

In Cloud Shell, copy the source code from the Cloud Shell command line:
gsutil cp -r gs://spls/gsp021/\* .
Copied!
Change into the directory needed for this lab:
cd orchestrate-with-kubernetes/kubernetes
Copied!
List the files to see what you're working with:
ls

## Task 2. A quick Kubernetes demo

The easiest way to get started with Kubernetes is to use the kubectl create command.

Use it to launch a single instance of the nginx container:
kubectl create deployment nginx --image=nginx:1.10.0
Copied!
Kubernetes has created a Deployment—more about Deployments later, but for now all you need to know is that Deployments keep the Pods up and running even when the nodes they run on fail.

In Kubernetes, all containers run in a Pod.

Use the kubectl get pods command to view the running nginx container:
kubectl get pods
Copied!
Once the nginx container has a Running status you can expose it outside of Kubernetes using the kubectl expose command:
kubectl expose deployment nginx --port 80 --type LoadBalancer
Copied!
So what just happened? Behind the scenes Kubernetes created an external load balancer with a public IP address attached to it. Any client who hits that public IP address will be routed to the Pods behind the service. In this case that would be the nginx Pod.

List the services now using the kubectl get services command:
kubectl get services
Copied!
Note: It may take a few seconds before the ExternalIP field is populated for your service. This is normal—just re-run the kubectl get services command every few seconds until the field populates.
Add the External IP to this command to hit the Nginx container remotely:
curl http://<External IP>:80
Copied!
And there you go! Kubernetes supports an easy to use workflow out of the box using the kubectl run and expose commands.

## Task 3. About Pods

At the core of Kubernetes is the Pod.

Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single Pod.

The following example shows a Pod that contains the monolith and nginx containers.

Pod containing the monolith and nginx containers

Pods also have Volumes. Volumes are data disks that live as long as the Pods live, and can be used by the containers in that Pod. Pods provide a shared namespace for their contents which means that the two containers inside of our example Pod can communicate with each other, and they also share the attached volumes.

Pods also share a network namespace. This means that there is one IP Address per Pod.

Next, a deeper dive into Pods.

## Task 4. Create Pods

Pods can be created using Pod configuration files. Take a moment to explore the monolith Pod configuration file.

Go to the directory:
cd ~/orchestrate-with-kubernetes/kubernetes
Copied!
Run the following to view a configuration file:
cat pods/monolith.yaml

There's a few things to notice here:

Your Pod is made up of one container (the monolith).
You're passing a few arguments to the container when it starts.
You're opening port 80 for http traffic.
Create the monolith Pod using kubectl:
kubectl create -f pods/monolith.yaml
Copied!
Examine your Pods. Use the kubectl get pods command to list all Pods running in the default namespace:
kubectl get pods
Copied!
Note: It may take a few seconds before the monolith Pod is up and running. The monolith container image needs to be pulled from the Docker Hub before you can run it.
Once the Pod is running, use the kubectl describe command to get more information about the monolith Pod:
kubectl describe pods monolith
Copied!
You see a lot of the information about the monolith Pod, including the Pod IP address and the event log. This information comes in handy when troubleshooting.

Kubernetes makes it easy to create Pods by describing them in configuration files, and then view information about them when they are running. At this point you have the ability to create all the Pods your Deployment requires!

## Task 5. Interact with Pods

By default, Pods are allocated a private IP address and cannot be reached outside the cluster. Use the kubectl port-forward command to map a local port to a port inside the monolith Pod.

Note: From this point on, this lab will ask you to work in multiple cloud shell tabs to set up communication between the Pods. Any commands that are executed in a second or third command shell will be denoted in the command's instructions.
Open a second Cloud Shell terminal. Now you have two terminals, one to run the kubectl port-forward command, and the other to issue curl commands.

In the 2nd terminal, run this command to set up port-forwarding:

kubectl port-forward monolith 10080:80
Copied!
Now in the 1st terminal start talking to your Pod using curl:
curl http://127.0.0.1:10080
Copied!
Yes! You got a friendly "hello" back from your container.

Now use the curl command to see what happens when you hit a secure endpoint:
curl http://127.0.0.1:10080/secure
Copied!
Uh oh.

To get an auth token back from the monolith, try logging in:
curl -u user http://127.0.0.1:10080/login
Copied!
At the login prompt, use the super-secret password password to login.
Logging in caused a JWT token to print out.

Since Cloud Shell does not handle copying long strings well, create an environment variable for the token.
TOKEN=$(curl http://127.0.0.1:10080/login -u user|jq -r '.token')
Copied!
Enter the super-secret password password again when prompted for the host password.

Use this command to copy and then use the token to hit the secure endpoint with curl:

curl -H "Authorization: Bearer $TOKEN" http://127.0.0.1:10080/secure
Copied!
At this point you should get a response back from the application, letting you know everything is right in the world again.

Use the kubectl logs command to view the logs for the monolith Pod.
kubectl logs monolith
Copied!
Open a 3rd terminal and use the -f flag to get a stream of the logs happening in real-time:
kubectl logs -f monolith
Copied!
Now if you use curl in the 1st terminal to interact with the monolith, you can see the logs updating (in the 3rd terminal):
curl http://127.0.0.1:10080
Copied!
Use the kubectl exec command to run an interactive shell inside the monolith Pod. This comes in handy when you want to troubleshoot from within a container:
kubectl exec monolith --stdin --tty -c monolith -- /bin/sh
Copied!
For example, once you have a shell into the monolith container you can test external connectivity using the ping command:
ping -c 3 google.com
Copied!
Be sure to log out when you're done with this interactive shell.
exit
Copied!
As you can see, interacting with Pods is as easy as using the kubectl command. If you need to hit a container remotely, or get a login shell, Kubernetes provides everything you need to get up and going.

## Task 6. About Services

Pods aren't meant to be persistent. They can be stopped or started for many reasons, like failed liveness or readiness checks, which leads to a problem:

Services are the persistent end point for the pods.

What happens if you want to communicate with a set of Pods? When they get restarted they might have a different IP address.

That's where Services come in. Services provide stable endpoints for Pods.

Services use labels to determine what Pods they operate on. If Pods have the correct labels, they are automatically picked up and exposed by our services.

The level of access a service provides to a set of Pods depends on the Service's type. Currently there are three types:

ClusterIP (internal) is the default type. This Service is only visible inside the cluster.
NodePort gives each node in the cluster an externally accessible IP.
LoadBalancer adds a load balancer from the cloud provider which forwards traffic from the Service to Nodes within it.

kubectl create secret generic tls-certs --from-file tls/
kubectl create configmap nginx-proxy-conf --from-file nginx/proxy.conf
kubectl create -f pods/secure-monolith.yaml

kubectl create -f services/monolith.yaml

gcloud compute firewall-rules create allow-monolith-nodeport \
 --allow=tcp:31000

## Deployment:

The Deployment creates 1 replica, and you're using version 2.0.0 of the auth container.

When you run the kubectl create command to create the auth Deployment it will make one Pod that conforms to the data in the Deployment manifest. This means you can scale the number of Pods by changing the number specified in the Replicas field.

Deployment, drive current state towards desired state

Anyway, go ahead and create your Deployment object:
kubectl create -f deployments/auth.yaml
Copied!
It's time to create a service for your auth Deployment. Use the kubectl create command to create the auth service:
kubectl create -f services/auth.yaml
Copied!
Do the same thing to create and expose the hello Deployment:
kubectl create -f deployments/hello.yaml
kubectl create -f services/hello.yaml
Copied!
And one more time to create and expose the frontend Deployment.
kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf
kubectl create -f deployments/frontend.yaml
kubectl create -f services/frontend.yaml
Copied!
Note: There is one more step to creating the frontend because you need to store some configuration data with the container.
Interact with the frontend by grabbing its External IP and then curling to it:
kubectl get services frontend
Copied!
Note: It might take a minute for the external IP address to be generated. Run the above command again if the EXTERNAL-IP column status is pending.
curl -k https://<EXTERNAL-IP>
Copied!
And you get a hello response back!
