Securing VM using BeyondCorpEnterprise(BCE)

## Multiple VPC network

Overview
Virtual Private Cloud (VPC) networks allow you to maintain isolated environments within a larger cloud structure, giving you granular control over data protection, network access, and application security.

In this lab you create several VPC networks and VM instances, then test connectivity across networks. Specifically, you create two custom mode networks (managementnet and privatenet) with firewall rules and VM instances as shown in this network diagram:

Network and subnets can be created using cloud shell command line:

gcloud compute networks create managementnet --project=qwiklabs-gcp-00-ca0a8f91046a --subnet-mode=custom --mtu=1460 --bgp-routing-mode=regional --bgp-best-path-selection-mode=legacy

gcloud compute networks subnets create managementnetsubnet-1 --project=qwiklabs-gcp-00-ca0a8f91046a --range=10.130.0.0/20 --stack-type=IPV4_ONLY --network=managementnet --region=us-central1

Create the privatenet network using the Cloud Shell command line.

Run the following command to create the privatenet network:
gcloud compute networks create privatenet --subnet-mode=custom

Run the following command to create the privatesubnet-1 subnet:
gcloud compute networks subnets create privatesubnet-1 --network=privatenet --region=us-central1 --range=172.16.0.0/24

Run the following command to create the privatesubnet-2 subnet:
gcloud compute networks subnets create privatesubnet-2 --network=privatenet --region=europe-west1 --range=172.20.0.0/20

Run the following command to list the available VPC networks:
gcloud compute networks list

command line to create VPC Network > FireWall

gcloud compute --project=qwiklabs-gcp-00-ca0a8f91046a firewall-rules create managementnet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=managementnet --action=ALLOW --rules=tcp:22,tcp:3389 --source-ranges=0.0.0.0/0

In Cloud Shell, run the following command to create the privatenet-allow-icmp-ssh-rdp firewall rule:

gcloud compute firewall-rules create privatenet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=privatenet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0

Run the following command to list all the firewall rules (sorted by VPC network):
gcloud compute firewall-rules list --sort-by=NETWORK

Task 2. Create VM instances
Create two VM instances:

managementnet-vm-1 in managementsubnet-1
privatenet-vm-1 in privatesubnet-1

Create the privatenet-vm-1 instance using the Cloud Shell command line.

In Cloud Shell, run the following command to create the privatenet-vm-1 instance:
gcloud compute instances create privatenet-vm-1 --zone=us-central1-c --machine-type=e2-micro --subnet=privatesubnet-1

Run the following command to list all the VM instances (sorted by zone):
gcloud compute instances list --sort-by=ZONE

Task 4. Create a VM instance with multiple network interfaces
Every instance in a VPC network has a default network interface. You can create additional network interfaces attached to your VMs. Multiple network interfaces enable you to create configurations in which an instance connects directly to several VPC networks (up to 8 interfaces, depending on the instance's type).

Create the VM instance with multiple network interfaces
Create the vm-appliance instance with network interfaces in privatesubnet-1, managementsubnet-1 and mynetwork. The CIDR ranges of these subnets do not overlap, which is a requirement for creating a VM with multiple network interface controllers (NICs).

In the Cloud console, navigate to Navigation menu > Compute Engine > VM instances.

Click Create Instance.

In the Machine configuration:

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name vm-appliance
Region us-central1
Zone us-central1-c
Series E2
Machine Type e2-standard-4
Note: The number of interfaces allowed in an instance is dependent on the instance's machine type and the number of vCPUs. The e2-standard-4 allows up to 4 network interfaces. Refer to the Maximum number of network interfaces section of the Google Cloud Guide for more information.
Click Networking.

For Network interfaces, click the dropdown to edit. Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Network privatenet
Subnetwork privatesubnet-1
Click Done.

Click Add a network interface.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Network managementnet
Subnetwork managementsubnet-1
Click Done.

Click Add a network interface.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Network mynetwork
Subnetwork mynetwork
Click Done.

Click Create.

////////////////

Install nginx and customize the welcome page

sudo apt-get install nginx-light -y

Install nginx on both VM instances and modify the welcome page to distinguish the servers.

Still in the VM instances dialog, for blue, click SSH to launch a terminal and connect.

In the SSH terminal to blue, run the following command to install nginx:

sudo apt-get install nginx-light -y
Copied!
Open the welcome page in the nano editor:
sudo nano /var/www/html/index.nginx-debian.html
Copied!
Replace the <h1>Welcome to nginx!</h1> line with <h1>Welcome to the blue server!</h1>.
Press CTRL+o, ENTER, CTRL+x.
Verify the change:
cat /var/www/html/index.nginx-debian.html
C

Repeat the same steps for the green server:

For green, click SSH to launch a terminal and connect.
Install nginx:
sudo apt-get install nginx-light -y
Copied!
Open the welcome page in the nano editor:
sudo nano /var/www/html/index.nginx-debian.html
Copied!
Replace the <h1>Welcome to nginx!</h1> line with <h1>Welcome to the green server!</h1>.
Press CTRL+o, ENTER, CTRL+x.
Verify the change:
cat /var/www/html/index.nginx-debian.html

/////
create vm name test-vm using commandline:

gcloud compute instances create test-vm --machine-type=e2-micro --subnet=default --zone=us-east1-d

////////////////////////////

# Application Load Balancer with Cloud Armor

Overview
Google Cloud Application Load Balancing is implemented at the edge of Google's network in Google's points of presence (POP) around the world. User traffic directed to an Application Load Balancer enters the POP closest to the user and is then load balanced over Google's global network to the closest backend that has sufficient capacity available.

Cloud Armor IP allowlist/denylist enable you to restrict or allow access to your Application Load Balancer at the edge of the Google Cloud, as close as possible to the user and to malicious traffic. This prevents malicious users or traffic from consuming resources or entering your Virtual Private Cloud (VPC) networks.

In this lab, you configure an Application Load Balancer with global backends, as shown in the diagram below. Then, you stress test the Load Balancer and denylist the stress test IP with Cloud Armor.

In this lab, you learn how to perform the following tasks:

Create HTTP and health check firewall rules
Configure two instance templates
Create two managed instance groups
Configure an Application Load Balancer with IPv4 and IPv6
Stress test an Application Load Balancer
Denylist an IP address to restrict access to an Application Load Balancer

Task 1. Configure HTTP and health check firewall rules
Configure firewall rules to allow HTTP traffic to the backends and TCP traffic from the Google Cloud health checker.

Create the HTTP firewall rule
Create a firewall rule to allow HTTP traffic to the backends.

In the Cloud console, navigate to Navigation menu (Navigation menu icon) > VPC network > Firewall.

Notice the existing ICMP, internal, RDP, and SSH firewall rules.

Each Google Cloud project starts with the default network and these firewall rules.

Click Create Firewall Rule.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name default-allow-http
Network default
Targets Specified target tags
Target tags http-server
Source filter IPv4 Ranges
Source IPv4 ranges 0.0.0.0/0
Protocols and ports Specified protocols and ports, and then check TCP, type: 80
Make sure to include the /0 in the Source IPv4 ranges to specify all networks.

Click Create.
Create the health check firewall rules
Health checks determine which instances of a load balancer can receive new connections. For Application Load Balancing, the health check probes to your load balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.

Still in the Firewall policies page, click Create Firewall Rule.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name default-allow-health-check
Network default
Targets Specified target tags
Target tags http-server
Source filter IPv4 Ranges
Source IPv4 ranges 130.211.0.0/22, 35.191.0.0/16
Protocols and ports Specified protocols and ports, and then check TCP
Note: Make sure to enter the two Source IPv4 ranges one-by-one and press SPACE in between them.
Click Create.

Click Check my progress to verify the objective.
Assessment Completed!
Configure HTTP and health check firewall rules

Assessment Completed!

Task 2. Configure instance templates and create instance groups
A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the Application Load Balancer.

Configure the instance templates
An instance template is an API resource that you use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties.

Create one instance template for us-east4 and one for europe-west1.

In the Cloud console, go to Navigation menu (Navigation menu icon) > Compute Engine > Instance templates, and then click Create instance template.

For Name, type us-east4-template.

For Location, Select Global.

For Series, select E2.

For Machine Type, select e2-micro.

Click Advanced Options.

Click Networking. Set the following value and leave all other values at their defaults:

Property Value (type value or select option as specified)
Network tags http-server
Click default under Network interfaces. Set the following values and leave all other values at their defaults:

Property Value (type value or select option as specified)
Network default
Subnetwork default us-east4
Click Done.

The network tag http-server ensures that the HTTP and Health Check firewall rules apply to these instances.

Click the Management tab.

Under Metadata, click + ADD ITEM and specify the following:

Key Value
startup-script-url gs://cloud-training/gcpnet/httplb/startup.sh
The startup-script-url specifies a script that executes when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region, and zone of the VM instance. Feel free to explore this script.

Click Create.
Wait for the instance template to be created.
Now create another instance template for subnet-b by copying us-east4-template:

Click on us-east4-template and then click on the +CREATE SIMILAR option from the top.
For Name, type europe-west1-template.
Ensure Location is selected Global.
Click Advanced Options.
Click Networking.
Ensure http-server is added as a network tag.
In Network interfaces, for Subnetwork, select default (europe-west1).
Click Done.
Click Create.
Create the managed instance groups
Create a managed instance group in us-east4 and one in europe-west1.

Still in Compute Engine, click Instance groups in the left menu.

Click Create instance group.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name us-east4-mig (if required, remove extra space from the name)
Instance template us-east4-template
Location Multiple zones
Region us-east4
Minimum number of instances 1
Maximum number of instances 2
Autoscaling signals > Click dropdown > Signal type CPU utilization
Target CPU utilization 80, click Done.
Initialization period 45
Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load.

Click Create.
Now repeat the same procedure to create a second instance group for europe-west1-mig in europe-west1:

Click Create Instance group.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name europe-west1-mig
Instance template europe-west1-template
Location Multiple zones
Region europe-west1
Minimum number of instances 1
Maximum number of instances 2
Autoscaling signals > Click dropdown > Signal type CPU utilization
Target CPU utilization 80, click Done.
Initialization period 45
Click Create.

Click Check my progress to verify the objective.
Assessment Completed!
Configure instance templates and instance group

Assessment Completed!

Verify the backends
Verify that VM instances are being created in both regions and access their HTTP sites.

Still in Compute Engine, click VM instances in the left menu.

Notice the instances that start with us-east4-mig and europe-west1-mig.

These instances are part of the managed instance groups.

Click on the External IP of an instance of us-east4-mig.

You should see the Client IP (your IP address), the Hostname (starts with us-east4-mig) and the Server Location (a zone in us-east4).

Click on the External IP of an instance of europe-west1-mig.

You should see the Client IP (your IP address), the Hostname (starts with europe-west1-mig) and the Server Location (a zone in europe-west1).

Note: The Hostname and Server Location identifies where the Application Load Balancer sends traffic.

Which of these fields identify the region of the backend?
check
Server Location

Client IP
check
Hostname

Task 3. Configure the Application Load Balancer
Configure the Application Load Balancer to balance traffic between the two backends (us-east4-mig in us-east4 and europe-west1-mig in europe-west1), as illustrated in the network diagram:

Network diagram that illustrates load balancing

Start the configuration
In the Cloud console, click Navigation menu (Navigation menu icon) > click VIEW ALL PRODUCTS > Networking > Network Services > Load balancing.

click Create load balancer.

Under Application Load Balancer HTTP(S), click Next.

For Public facing or internal, select Public facing (external) and click Next.

For Global or single region deployment, select Best for global workloads and click Next.

For Create load balancer, click Configure.

Set Load Balancer Name to http-lb.

Configure the frontend
The host and path rules determine how your traffic will be directed. For example, you could direct video traffic to one backend and static traffic to another backend. However, you are not configuring the Host and path rules in this lab.

Click on Frontend configuration.

Specify the following, leaving all other values at their defaults:

Property Value (type value or select option as specified)
Protocol HTTP
IP version IPv4
IP address Ephemeral
Port 80
Click Done.

Click Add Frontend IP and port.

Specify the following, leaving all other values at their defaults:

Property Value (type value or select option as specified)
Protocol HTTP
IP version IPv6
IP address Auto-allocate
Port 80
Click Done.

Application Load Balancing supports both IPv4 and IPv6 addresses for client traffic. Client IPv6 requests are terminated at the global load balancing layer, then proxied over IPv4 to your backends.

Configure the backend
Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata.

Click on Backend configuration.

For Backend services & backend buckets, click Create a backend service.

Set the following values, leave all other values at their defaults:

Property Value (select option as specified)
Name http-backend
Instance group us-east4-mig
Port numbers 80
Balancing mode Rate
Maximum RPS 50
Capacity 100
This configuration means that the load balancer attempts to keep each instance of us-east4-mig at or below 50 requests per second (RPS).

Click Done.

Click Add a backend.

Set the following values, leave all other values at their defaults:

Property Value (select option as specified)
Instance group europe-west1-mig
Port numbers 80
Balancing mode Utilization
Maximum backend utilization 80
Capacity 100
This configuration means that the load balancer attempts to keep each instance of europe-west1-mig at or below 80% CPU utilization.

Click Done.

For Health Check, select Create a health check.

Set the following values, leave all other values at their defaults:

Property Value (select option as specified)
Name http-health-check
Protocol TCP
Port 80
Health checks determine which instances receive new connections. This HTTP health check polls instances every 5 seconds, waits up to 5 seconds for a response and treats 2 successful or 2 failed attempts as healthy or unhealthy, respectively.

Click Save.
Check the Enable Logging box.
Set the Sample Rate to 1.
Click Create to create the backend service.
Click Ok.
Review and create the Application Load Balancer
Click on Review and finalize.
Review the Backend and Frontend services.
Click on Create.
Wait for the load balancer to be created.
Click on the name of the load balancer (http-lb).
Note the IPv4 and IPv6 addresses of the load balancer for the next task. They will be referred to as [LB_IP_v4] and [LB_IP_v6], respectively.
Note: The IPv6 address is the one in hexadecimal format.
Click Check my progress to verify the objective.
Assessment Completed!
Configure the Application Load Balancer

Assessment Completed!

Task 4. Test the Application Load Balancer
Now that you created the Application Load Balancer for your backends, verify that traffic is forwarded to the backend service.

The Application Load Balancer should forward traffic to the region that is closest to you.
check
True
False

Access the Application Load Balancer
To test IPv4 access to the Application Load Balancer, open a new tab in your browser and navigate to http://[LB_IP_v4]. Make sure to replace [LB_IP_v4] with the IPv4 address of the load balancer.

Note: It might take up to 5 minutes to access the Application Load Balancer. In the meantime, you might get a 404 or 502 error. Keep trying until you see the page of one of the backends.
Note: Depending on your proximity to us-east4 and europe-west1, your traffic is either forwarded to a us-east4-mig or europe-west1-mig instance.
If you have a local IPv6 address, try the IPv6 address of the Application Load Balancer by navigating to http://[LB_IP_v6]. Make sure to replace [LB_IP_v6] with the IPv6 address of the load balancer.

Stress test the Application Load Balancer
Create a new VM to simulate a load on the Application Load Balancer using siege. Then, determine if traffic is balanced across both backends when the load is high.

In the console, navigate to Navigation menu (Navigation menu icon) > Compute Engine > VM instances.

Click Create instance.

In the Machine configuration:

Select the following values:

Property Value (type value or select option as specified)
Name siege-vm
Region us-east1
Zone us-east1-c
Series E2
Given that us-east1 is closer to us-east4 than to europe-west1, traffic should be forwarded only to us-east4-mig (unless the load is too high).

Click Create.
Wait for the siege-vm instance to be created.
For siege-vm, click SSH to launch a terminal and connect.
Run the following command, to install siege:
sudo apt-get -y install siege
Copied!
To store the IPv4 address of the Application Load Balancer in an environment variable, run the following command, replacing [LB_IP_v4] with the IPv4 address:
export LB_IP=[LB_IP_v4]
Copied!
To simulate a load, run the following command:
siege -c 150 -t120s http://$LB_IP
Copied!
In the Cloud console, click Navigation menu (Navigation menu icon) > click VIEW ALL PRODUCTS > Networking > Network Services > Load balancing.

Click Backends.

Click http-backend.

Navigate to http-lb.

Click on the Monitoring tab.

Monitor the Frontend Location (Total inbound traffic) between North America and the two backends for 2 to 3 minutes.

At first, traffic should just be directed to us-east4-mig but as the RPS increases, traffic is also directed to europe-west1.

This demonstrates that by default traffic is forwarded to the closest backend but if the load is very high, traffic can be distributed across the backends.

Return to the SSH terminal of siege-vm.
Press CTRL+C to stop siege if it's still running.
The output should look like this:

New configuration template added to /home/student-02-dd02c94b8808/.siege
Run siege -C to view the current settings in that file
{ "transactions": 24729,
"availability": 100.00,
"elapsed_time": 119.07,
"data_transferred": 3.77,
"response_time": 0.66,
"transaction_rate": 207.68,
"throughput": 0.03,
"concurrency": 137.64,
"successful_transactions": 24729,
"failed_transactions": 0,
"longest_transaction": 10.45,
"shortest_transaction": 0.03
}
Task 5. Denylist the siege-vm
Use Cloud Armor to denylist the siege-vm from accessing the Application Load Balancer.

Create the security policy
Create a Cloud Armor security policy with a denylist rule for the siege-vm.

In the console, navigate to Navigation menu (Navigation menu icon) > Compute Engine > VM instances.
Note the External IP of the siege-vm. This will be referred to as [SIEGE_IP].
Note: There are ways to identify the external IP address of a client trying to access your Application Load Balancer. For example, you could examine traffic captured by VPC Flow Logs in BigQuery to determine a high volume of incoming requests.
In the Cloud console, click Navigation menu (Navigation menu icon) > click VIEW ALL PRODUCTS > Networking > Network Security > Cloud Armor policies.

Click Create policy.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name denylist-siege
Default rule action Allow
Click Next step.

Click Add a rule.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Condition > Match Enter the SIEGE_IP
Action Deny
Response code 403 (Forbidden)
Priority 1000
Click Save Change to Rule.

Click Next step.

Click Add Target.

For Type, select Backend service (external application load balancer).

For Target, select http-backend and if prompted confirm Replace.

Click Create policy.

Note: Alternatively, you could set the default rule to Deny and only allowlist or allow traffic from authorized users/IP addresses.
Wait for the policy to be created before moving to the next step.
Click Check my progress to verify the objective.
Assessment Completed!
Denylist the siege-vm

Assessment Completed!

Verify the security policy
Verify that the siege-vm cannot access the Application Load Balancer.

Return to the SSH terminal of siege-vm.
To access the load balancer, run the following:
curl http://$LB_IP
Copied!
The output should look like this:

<!doctype html><meta charset="utf-8"><meta name=viewport content="width=device-width, initial-scale=1"><title>403</title>403 Forbidden

Note: It might take a couple of minutes for the security policy to take effect. If you are able to access the backends, keep trying until you get the 403 Forbidden error.
Open a new tab in your browser and navigate to http://[LB_IP_v4]. Make sure to replace [LB_IP_v4] with the IPv4 address of the load balancer.
Note: You can access the Application Load Balancer from your browser because of the default rule to allow traffic; however, you cannot access it from the siege-vm because of the deny rule that you implemented.
Back in the SSH terminal of siege-vm, to simulate a load, run the following command:
siege -c 150 -t120s http://$LB_IP
Copied!
The command will not generate any output.

Explore the security policy logs to determine if this traffic is also blocked.

In the console, navigate to Navigation menu > Network Security > Cloud Armor Policies.
Click denylist-siege.
Click Logs.
Click View policy logs.
On the Logging page, make sure to clear all the text in the Query preview. Select resource to Application Load Balancer > http-lb-forwarding-rule > http-lb then click Apply.
Now click Run Query.
Expand a log entry in Query results.
Expand httpRequest.
The request should be from the siege-vm IP address. If not, expand another log entry.

Expand jsonPayload.
Expand enforcedSecurityPolicy.
Notice that the configuredAction is to DENY with the name denylist-siege.
Query results page

Cloud Armor security policies create logs that can be explored to determine when traffic is denied and when it is allowed, along with the source of the traffic.

////////////////////////////

# Create an Internal Load Balancer

Google Cloud offers Internal Load Balancing for your TCP/UDP-based traffic. Internal Load Balancing enables you to run and scale your services behind a private load balancing IP address that is accessible only to your internal virtual machine instances.

In this lab you create two managed instance groups in the same region. Then, you configure and test an Internal Load Balancer with the instances groups as the backends, as shown in this network diagram:

You will create the managed instance groups in subnet-a and subnet-b. Both subnets are in the us-west1 region because an Internal Load Balancer is a regional service. The managed instance groups will be in different zones, making your service immune to zonal failures.

## Challenge Lab:

For the firewall rules, make sure that:

The bastion host does not have a public IP address.
You can only SSH to the bastion and only via IAP.
You can only SSH to juice-shop via the bastion.
Only HTTP is open to the world for juice-shop.

The bastion host is the one machine authorized to receive external SSH traffic. Create a firewall rule that allows SSH (tcp/22) from the IAP service. The firewall rule must be enabled for the bastion host instance using a network tag of accept-ssh-iap-ingress-ql-503.

gcloud compute firewall-rules create accept-ssh-iap-ingress-ql-503 \
 --allow tcp:22 \
 --source-ranges=35.235.240.0/20 \
 --target-tags=bastion-host \
 --direction=INGRESS \
 --priority=1000 \
 --network=default \
 --description="Allow SSH from IAP to bastion host"

gcloud compute instances add-tags <INSTANCE_NAME> \
 --tags=allow-iap-ssh \
 --zone=<ZONE>

gcloud compute instances add-tags bastion \
 --tags=accept-ssh-iap-ingress-ql-503 \
 --zone=us-central1-c

gcloud compute instances add-tags bastion \
 --tags bastion-host \
 --zone us-central1-c

gcloud compute firewall-rules create accept-ssh-iap-ingress-ql-503 \
 --direction=INGRESS \
 --action=allow \
 --rules=tcp:22 \
 --source-ranges=35.235.240.0/20

gcloud compute firewall-rules create allow-ssh-from-iap-to-bastion \
 --direction=INGRESS \
 --priority=1000 \
 --network=acme-vpc \
 --action=ALLOW \
 --rules=tcp:22 \
 --source-ranges=35.235.240.0/20 \
 --target-tags=accept-ssh-iap-ingress-ql-503 \
 --description="Allow SSH from IAP to bastion host only"

gcloud compute instances add-tags bastion \
 --tags=accept-ssh-iap-ingress-ql-503 \
 --zone=us-central1-c

gcloud compute firewall-rules create allow-http-to-juice-shop \
 --direction=INGRESS \
 --priority=1000 \
 --network=acme-vpc \
 --action=ALLOW \
 --rules=tcp:80 \
 --source-ranges=0.0.0.0/0 \
 --target-tags=accept-http-ingress-ql-503 \
 --description="Allow HTTP traffic from any source to juice-shop instance"

gcloud compute instances add-tags juice-shop \
 --tags=accept-http-ingress-ql-503 \
 --zone=us-central1-c

gcloud compute networks subnets describe acme-mgmt-subnet \
 --region=us-central1

gcloud compute firewall-rules create allow-ssh-from-acme-mgmt-subnet \
 --direction=INGRESS \
 --priority=1000 \
 --network=acme-vpc \
 --action=ALLOW \
 --rules=tcp:22 \
 --source-ranges=192.168.10.0/24 \
 --target-tags=accept-ssh-internal-ingress-ql-503 \
 --description="Allow SSH from acme-mgmt-subnet to juice-shop instance"

gcloud compute instances add-tags juice-shop \
 --tags=accept-ssh-internal-ingress-ql-503 \
 --zone=us-central1-c

--network=acme-vpc

//////////////////////////

Learning contents:

Objectives
In this lab you learn how to perform the following tasks:

Create HTTP and health check firewall rules
Configure two instance templates
Create two managed instance groups
Configure and test an internal load balancer

Task 1. Configure HTTP and health check firewall rules
Configure firewall rules to allow HTTP traffic to the backends and TCP traffic from the Google Cloud health checker.

Explore the my-internal-app network
The network my-internal-app with subnet-a and subnet-b along with firewall rules for RDP, SSH, and ICMP traffic have been configured for you.

In the console, navigate to Navigation menu > VPC network > VPC networks.

Scroll down and notice the my-internal-app network with its subnets: subnet-a and subnet-b

Each Google Cloud project starts with the default network. In addition, the my-internal-app network has been created for you, as part of your network diagram.

You will create the managed instance groups in subnet-a and subnet-b. Both subnets are in the us-east1 region because an Internal Load Balancer is a regional service. The managed instance groups will be in different zones, making your service immune to zonal failures.

Create the HTTP firewall rule
Create a firewall rule to allow HTTP traffic to the backends from the Load Balancer and the internet (to install Apache on the backends).

Still in VPC network, in the left pane click Firewall.

Notice the app-allow-icmp and app-allow-ssh-rdp firewall rules.

These firewall rules have been created for you.

Click + Create Firewall Rule.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name app-allow-http
Network my-internal-app
Targets Specified target tags
Target tags lb-backend
Source filter IPv4 Ranges
Source IPv4 ranges 10.10.0.0/16
Protocols and ports Specified protocols and ports, and then check tcp, type: 80
Note: Make sure to include the /16 in the Source IPv4 ranges to specify all networks.
Click Create.
Create the health check firewall rules
Health checks determine which instances of a Load Balancer can receive new connections. For Internal load balancing, the health check probes to your load balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.

Still in the Firewall rules page, click + Create Firewall Rule.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name app-allow-health-check
Network my-internal-app
Targets Specified target tags
Target tags lb-backend
Source filter IPv4 Ranges
Source IPv4 ranges 130.211.0.0/22 and 35.191.0.0/16
Protocols and ports Specified protocols and ports, and then check tcp
Note: Make sure to enter the two Source IPv4 ranges one-by-one and pressing SPACE in between them.
Click Create.
Click Check my progress to verify the objective.

Assessment Completed!
Configure HTTP and health check firewall rules

Assessment Completed!
Task 2. Configure instance templates and create instance groups
A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the Internal Load Balancer.

Configure the instance templates
An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. Create an instance template for both subnets of the my-internal-app network.

In the Console, navigate to Navigation menu > Compute Engine > Instance templates.

Click Create instance template.

For Name, type instance-template-1.

For Location, Select Global.

For Series, select E2.

For Machine type, select Shared-core > e2-micro.

Click Advanced options.

Click Networking.

For Network tags, specify lb-backend.

Note: The network tag lb-backend ensures that the HTTP and Health Check firewall rules apply to these instances.
For Network interfaces, click the dropdown icon to edit.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Network my-internal-app
Subnetwork subnet-a
External IPv4 Address None
Click Done.

Click Management.

Under Metadata, click Add item and specify the following:

Key 1 Value 1
startup-script-url gs://cloud-training/gcpnet/ilb/startup.sh
Note: The startup-script-url specifies a script that will be executed when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region and zone of the VM instance. Feel free to explore this script.
Click Create.
Wait for the instance template to be created.
Configure the next instance template
Create another instance template for subnet-b by copying instance-template-1:

Still in Instance templates, check the box next to instance-template-1, then click Copy. Make sure to update the name as instance-template-2.
Click Advanced options.
Click the Networking tab.
For Network interfaces, click the dropdown icon to edit.
Select subnet-b as the Subnetwork.
Click Done and then click Create.
Create the managed instance groups
Create a managed instance group in subnet-a and one subnet-b.

Note: Identify one of the other zones in the same region as subnet-a. For example, if your zone of subnet-a is us-west2-a, you could select us-west2-b for subnet-b.
Still in Compute Engine, in the left pane click Instance groups, and then click Create Instance group.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name instance-group-1
Instance template instance-template-1
Location Single-zone
Region us-east1
Zone us-east1-d
Autoscaling > Minimum number of instances 1
Autoscaling > Maximum number of instances 1
Autoscaling > Autoscaling signals (click the dropdown icon to edit) > Signal type CPU utilization
Target CPU utilization 80
Initialization period 45
Note: Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load.
Click Create.

Repeat the same procedure for instance-group-2 in the different zone of same region as subnet-a:

Click Create Instance group.

Set the following values, leave all other values at their defaults:

Property Value (type value or select option as specified)
Name instance-group-2
Instance template instance-template-2
Location Single-zone
Region us-east1
Zone Zone (Use the different zone in same region as subnet-a)
Autoscaling > Minimum number of instances 1
Autoscaling > Maximum number of instances 1
Autoscaling > Autoscaling signals (click the dropdown icon to edit) > Signal type CPU utilization
Target CPU utilization 80
Initialization period 45
Click Create.

Verify the backends
Verify that VM instances are being created in both subnets and create a utility VM to access the backends' HTTP sites.

Still in Compute Engine, click VM instances.

Notice two instances that start with instance-group-1 and instance-group-2.

These instances are in separate zones and their internal IP addresses are part of the subnet-a and subnet-b CIDR blocks.

To create a new instance, click Create Instance.

In the Machine configuration.

Select the following values:

Property Value (type value or select option as specified)
Name utility-vm
Region us-east1
Zone us-east1-d
Series E2
Machine Type e2-micro (1 shared vCPU)
Click Networking.

For Network interfaces, click Toggle to Edit network interface.

Specify the following:

Property Value (type value or select option as specified)
Network my-internal-app
Subnetwork subnet-a
Primary internal IPv4 address Ephemeral (Custom)
Custom ephemeral IP address 10.10.20.50
Click Done and then click Create.

Click Check my progress to verify the objective.

Assessment Completed!
Configure instance templates and create instance groups

Assessment Completed!
Note that the internal IP addresses for the backends are 10.10.20.2 and 10.10.30.2.
Note: If these IP addresses are different, replace them in the two curl commands below.
For utility-vm, click SSH to launch a terminal and connect.
To verify the welcome page for instance-group-1-xxxx, run the following command:
curl 10.10.20.2
Copied!
The output should look like this:

<h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname:
 instance-group-1-1zn8<h2>Server Location</h2>Region and Zone: us-central1-a
To verify the welcome page for instance-group-2-xxxx, run the following command:
curl 10.10.30.2
Copied!
The output should look like this:

<h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname:
 instance-group-2-q5wp<h2>Server Location</h2>Region and Zone: us-central1-b

Which of these fields identify the location of the backend?

Client IP
check
Server Hostname
check
Server Location

Note: The curl commands demonstrate that each VM instance lists the Client IP and its own name and location. This will be useful when verifying that the Internal Load Balancer sends traffic to both backends.
Close the SSH terminal to utility-vm:
exit
Copied!
Task 3. Configure the Internal Load Balancer
Configure the Internal Load Balancer to balance traffic between the two backends (instance-group-1 and instance-group-2), as illustrated in this diagram:

Network Diagram showing the Internal Load Balancer balancing traffic between the 2 backends

Start the configuration
From the Navigation Menu, select View All Products. Under Networking, select Network Services.
Select the Load balancing page.
Click Create load balancer.
For Type of load balancer, select Network Load Balancer (TCP/UDP/SSL).
For Proxy or passthrough, select Passthrough load balancer.
For Public facing or internal, select Internal.
Click CONFIGURE.
For Name, type my-ilb.
For Region, select us-east1.
For Network, select my-internal-app.
Configure the regional backend service
The backend service monitors instance groups and prevents them from exceeding configured usage.

Click on Backend configuration.

Set the following values, leave all other values at their defaults:

Property Value (select option as specified)
Instance group instance-group-1
Click Add a backend.

For Instance group, select instance-group-2.

For Health Check, select Create a health check.

Set the following values, leave all other values at their defaults:

Property Value (select option as specified)
Name my-ilb-health-check
Protocol TCP
Port 80
Note: Health checks determine which instances can receive new connections. This HTTP health check polls instances every 5 seconds, waits up to 5 seconds for a response and treats 2 successful or 2 failed attempts as healthy or unhealthy, respectively.
Click Save.
Verify that there is a blue check mark next to Backend configuration in the Cloud Console. If not, double-check that you have completed all the steps above.
Configure the frontend
The frontend forwards traffic to the backend.

Click on Frontend configuration.

Specify the following, leaving all other values with their defaults:

Property Value (type value or select option as specified)
Subnetwork subnet-b
Internal IP Under IP address select Create IP address
Specify the following, leaving all other values with their defaults:

Property Value (type value or select option as specified)
Name my-ilb-ip
Static IP address Let me choose
Custom IP address 10.10.30.5
Click Reserve.

In Port number, type 80.

Click Done .

Review and create the Internal Load Balancer
Click on Review and finalize.

Review the Backend and Frontend.

Click on Create. Wait for the Load Balancer to be created, before moving to the next task.

Click Check my progress to verify the objective.

Assessment Completed!
Configure the Internal Load Balancer

Assessment Completed!
Task 4. Test the Internal Load Balancer
Verify that the my-ilb IP address forwards traffic to instance-group-1 and instance-group-2.

Access the Internal Load Balancer
In the Cloud Console, navigate to Navigation menu > Compute Engine > VM instances.
For utility-vm, click SSH to launch a terminal and connect.
To verify that the Internal Load Balancer forwards traffic, run the following command:
curl 10.10.30.5
Copied!
The output should look like this:

<h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname:
 instance-group-1-1zn8<h2>Server Location</h2>Region and Zone: us-central1-a
Note: As expected, traffic is forwarded from the Internal Load Balancer (10.10.30.5) to the backend.
Run the same command a couple more times.
In the output, you should be able to see responses from instance-group-1 in us-east1-d and instance-group-2 in the different zone of same region.

Congratulations!
In this lab you created two managed instance groups in the us-east1 region, along with firewall rules to allow HTTP traffic to those instances and TCP traffic from the Google Cloud health checker. Then, you configured and tested an Internal Load Balancer for those instance groups.

////////////////

Objectives
In this lab you learn how to perform the following tasks:

Create HTTP and health check firewall rules
Configure two instance templates ----> 2 instance-template-1/2
Create two managed instance groups -----> 2 instance-group-1/2
Configure and test an internal load balancer
